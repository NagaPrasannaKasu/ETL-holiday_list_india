Here I came up with a small project building an ETL-process based on API and Dockerized it.
This is to prepare myself to have an overview on how to build a simple data pipeline and run it in a container. 

API origin -> https://calendarific.com/
An API data contains list of public holidays from different countries. The data is in Json format which contains several fields that are semi-structured.
My goal is to extract list of holidays from a specific year and from a specific country then validate the data and then load the data into the database.
Complete ETL process is done using PYTHON programming language and to load the data into the database, SQLite extension is used in VSCode. 
For my convenience I used Git-Bash to run the Program. Then finally the source code ran in the docker container.

Infrastructure:
OS: Windows 10
platform: Visual Studio Code 1.62
programming language : Python
libraries used: sqlite3, requests, pandas
used Git-Bash to execute Python program as well as to initialize Docker container

Docker version :
Client:
 Cloud integration: v1.0.20
 Version:           20.10.10
 API version:       1.41
 Go version:        go1.16.9
 Git commit:        b485636
 Built:             Mon Oct 25 07:47:53 2021
 OS/Arch:           windows/amd64
 Context:           default
 Experimental:      true

Server: Docker Engine - Community
 Engine:
  Version:          20.10.10
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.16.9
  Git commit:       e2f740d
  Built:            Mon Oct 25 07:41:30 2021
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.4.11
  GitCommit:        5b46e404f6b9f661a205e28d59c982d3634148f8
 runc:
  Version:          1.0.2
  GitCommit:        v1.0.2-0-g52b36a2
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0

DockerHub: 
https://hub.docker.com/repository/docker/prasannakasu/holiday_list
https://hub.docker.com/repository/docker/prasannakasu/country_holidays_docker
